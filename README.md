# Case Study 2: Evidence Collection

**"What Companies Say vs. What They Do"**

**Course:** Big Data and Intelligent Analytics  
**Instructor:** Sri Krishnamurthy â€” QuantUniversity  
**Term:** Spring 2026

**Team 3:**
- Vaishnavi Srinivas
- Ishaan Samel
- Ayush Fulsundar

---

## ğŸ§  Project Overview

This project implements the **Evidence Collection layer** of the PE-OrgAIR platform. Building on **Case Study 1 (Platform Foundation)**, this case study focuses on ingesting, processing, and persisting **verifiable evidence** that reflects a company's **actual AI investment**, not just public claims.

### Evidence Types

We collect and store two types of evidence:

1. **What companies say** â†’ SEC filings (10-K, 10-Q, 8-K)
2. **What companies do** â†’ External signals (jobs, tech stack, patents, leadership)

All evidence is normalized, scored, and persisted in **Snowflake**, forming the foundation for AI-readiness scoring in future case studies.

---

## âš–ï¸ System Architecture

### High-level Flow
```
External Sources
â”œâ”€â”€ SEC EDGAR (10-K, 10-Q, 8-K)
â”œâ”€â”€ Job Boards (Indeed, Google Jobs)
â”œâ”€â”€ Technology Stack (BuiltWith / SimilarTech)
â”œâ”€â”€ Patents (USPTO - mock)
â””â”€â”€ Leadership Profiles (manual / CSV / mock)
    â†“
Evidence Collection Pipelines
    â†“
Snowflake (Documents, Chunks, Signals, Summaries)
```

### Key Design Principle

**SEC filings capture *intent*, while external signals capture *execution*.**

---

## ğŸ“‚ Project Structure
```
PE_OrgAIR_CaseStudy2/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ deps.py                     # Dependency injection
â”‚   â”‚
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ schema.sql                  # Core schema
â”‚   â”‚   â””â”€â”€ schema_case_study_2.sql     # CS2-specific tables
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ assessment.py               # Assessment data models
â”‚   â”‚   â”œâ”€â”€ company.py                  # Company entities
â”‚   â”‚   â”œâ”€â”€ dimension.py                # Scoring dimensions
â”‚   â”‚   â”œâ”€â”€ document.py                 # SEC filing models
â”‚   â”‚   â”œâ”€â”€ evidence.py                 # Evidence structures
â”‚   â”‚   â”œâ”€â”€ industry.py                 # Industry classifications
â”‚   â”‚   â””â”€â”€ signal.py                   # External signals
â”‚   â”‚
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ sec_edgar.py                # SEC EDGAR data ingestion
â”‚   â”‚   â”œâ”€â”€ document_parser_from_s3.py  # Parse docs from S3
â”‚   â”‚   â”œâ”€â”€ document_text_cleaner.py    # Text preprocessing
â”‚   â”‚   â”œâ”€â”€ document_chunker_s3.py      # Semantic chunking
â”‚   â”‚   â”œâ”€â”€ job_signals.py              # Job posting scraper
â”‚   â”‚   â”œâ”€â”€ tech_signals.py             # Tech stack detection
â”‚   â”‚   â”œâ”€â”€ patent_signals.py           # Patent analysis
â”‚   â”‚   â”œâ”€â”€ leadership_signals.py       # Leadership scoring
â”‚   â”‚   â””â”€â”€ external_signals_orchestrator.py  # Signal coordinator
â”‚   â”‚
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ companies.py                # Company endpoints
â”‚   â”‚   â”œâ”€â”€ assessments.py              # Assessment APIs
â”‚   â”‚   â”œâ”€â”€ dimension.py                # Dimension management
â”‚   â”‚   â”œâ”€â”€ documents.py                # Document retrieval
â”‚   â”‚   â”œâ”€â”€ signals.py                  # Signal endpoints
â”‚   â”‚   â””â”€â”€ health.py                   # Health checks
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ snowflake.py                # Snowflake connector
â”‚   â”‚   â”œâ”€â”€ s3_storage.py               # S3 operations
â”‚   â”‚   â””â”€â”€ redis_cache.py              # Redis caching layer
â”‚   â”‚
â”‚   â”œâ”€â”€ streamlit_app/
â”‚   â”‚   â””â”€â”€ app.py                      # Dashboard UI
â”‚   â”‚
â”‚   â”œâ”€â”€ config.py                       # Configuration management
â”‚   â””â”€â”€ main.py                         # FastAPI entrypoint
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_sec_edgar.py                # Execute SEC pipeline
â”‚   â”œâ”€â”€ run_external_signals.py         # Run signal collection
â”‚   â”œâ”€â”€ parse_document.py               # Parse individual docs
â”‚   â”œâ”€â”€ clean_documents_from_s3.py      # Clean S3 documents
â”‚   â”œâ”€â”€ chunk_documents_from_s3.py      # Chunk S3 documents
â”‚   â”œâ”€â”€ backfill_companies.py           # Populate company data
â”‚   â””â”€â”€ company_uspto_names.py          # USPTO name mapping
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                            # Raw downloaded data
â”‚   â”œâ”€â”€ processed/                      # Processed outputs
â”‚   â””â”€â”€ samples/                        # Sample datasets
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ evidence_report.md              # Analysis & findings
â”‚
â”œâ”€â”€ tests/                              # Unit & integration tests
â”œâ”€â”€ Dockerfile                          # Container definition
â”œâ”€â”€ docker-compose.yml                  # Multi-service orchestration
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ pyproject.toml                      # Poetry configuration
â””â”€â”€ README.md                           # Project documentation
```

### Key Components

#### ğŸ”§ **Core Application** (`app/`)
- **Models**: Pydantic schemas for data validation
- **Pipelines**: ETL workflows for evidence collection
- **Routers**: RESTful API endpoints
- **Services**: External system integrations (Snowflake, S3, Redis)

#### ğŸ“œ **Scripts** (`scripts/`)
Standalone executables for:
- Data ingestion and processing
- Pipeline orchestration
- Database backfilling

#### ğŸ—„ï¸ **Data** (`data/`)
- **raw/**: Unprocessed source files
- **processed/**: Cleaned and transformed data
- **samples/**: Test datasets

#### ğŸ³ **Infrastructure**
- **Docker**: Containerized deployment
- **docker-compose**: Local development stack

## ğŸ“Š Evidence Pipelines Implemented

### 1ï¸âƒ£ SEC EDGAR Pipeline (Lab 3)

- Downloads **10-K, 10-Q, 8-K** filings for 10 target companies
- Supports **PDF and HTML** formats
- Extracts AI-relevant sections:
  - Item 1 â€“ Business
  - Item 1A â€“ Risk Factors
  - Item 7 â€“ MD&A
- Implements **semantic chunking with overlap**
- Deduplicates documents using **SHA-256 content hashing**
- Tracks document lifecycle via a **document registry**

**Stored in:**
- `documents`
- `document_chunks`

---

### 2ï¸âƒ£ External Signals Pipeline (Lab 4)

#### ğŸ”¹ Technology Hiring Signals

- Scrapes job postings from **Indeed & Google Jobs**
- Filters AI-related roles using keyword and skill heuristics
- Normalizes hiring intensity to a **0â€“100 score**
- Handles company aliases (e.g., JPMorgan, Chase, JPMC)

#### ğŸ”¹ Digital Presence Signals

- Detects AI-related technologies (ML frameworks, cloud ML, AI APIs)
- Scores based on:
  - Number of AI technologies
  - Coverage across AI categories

#### ğŸ”¹ Innovation / Patent Signals

- Mock USPTO ingestion
- Scores AI patent volume, recency, and category diversity

#### ğŸ”¹ Leadership Signals

- Executive-level AI commitment scoring
- Uses role-weighted and indicator-based scoring
- One signal per executive, aggregated at company level

**Stored in:**
- `external_signals`
- `company_signal_summaries`

---

## ğŸ—„ï¸ Data Persistence (Snowflake)

### Core Tables

- `documents`
- `document_chunks`
- `external_signals`
- `company_signal_summaries`

### Key Guarantees

- All signals stored with rich metadata (JSON VARIANT)
- Scores normalized to **0â€“100**
- Composite score computed using weighted aggregation
- Signals traceable to source and timestamp

---

## ğŸ“ˆ Scoring Model

| Signal Category | Weight |
|----------------|--------|
| Technology Hiring | 0.30 |
| Innovation Activity | 0.25 |
| Digital Presence | 0.25 |
| Leadership Signals | 0.20 |

**Composite Score = weighted sum of all four categories.**

---

## â–¶ï¸ How to Run

### Run External Signals for a Company
```bash
poetry run python scripts/run_external_signals.py \
  --company-id <UUID> \
  --query "machine learning engineer" \
  --location "United States" \
  --sources indeed,google \
  --max-per-source 25
```

### Verify Data in Snowflake
```sql
SELECT * FROM external_signals;
SELECT * FROM company_signal_summaries;
```

---

## ğŸ“„ Evidence Report

View the complete analysis and findings:

[Evidence Collection Report](https://docs.google.com/document/d/1uM8F2Y0ZmF4nhfrEKaGMd3pm_phT4vguyAot3XAxEt4/edit?tab=t.0)

The report includes:
- Company-wise document counts
- Signal scores by category
- Composite scores
- Observed "say vs do" gaps
- Data quality notes

## ğŸ¯ Next Steps

This evidence layer feeds into **Case Study 3: AI-Readiness Scoring**, where we'll build machine learning models to predict company AI maturity based on the collected evidence.

---

## ğŸ“¦ Requirements

See `requirements.txt` for full dependencies. Key packages:
- `snowflake-connector-python`
- `requests`
- `beautifulsoup4`
- `python-dotenv`
- `pandas`

---

## ğŸ‘¥ Team Contributions

- **Vaishnavi Srinivas** â€“ External signals orchestration
- **Ishaan Samel** â€“ Snowflake integration, data quality validation
- **Ayush Fulsundar** â€“  SEC EDGAR ingestion, document parsing, cleaning, and chunking

---
## ğŸ¥ Demo Video

Watch our project demonstration:

[![Demo Video](https://img.shields.io/badge/Watch-Demo%20Video-red?style=for-the-badge&logo=google-drive)](https://drive.google.com/drive/folders/1bNFGsU0ojkWythDrCrzsGkeT6hBsrS48)

[ğŸ“¹ View Demo Video on Google Drive](https://drive.google.com/drive/folders/1bNFGsU0ojkWythDrCrzsGkeT6hBsrS48)

### ğŸ“š Interactive Codelab

Follow our step-by-step interactive tutorial:

**[ğŸ“– Open Codelab: Evidence Collection - What Companies Say vs. What They Do](https://codelabs-preview.appspot.com/?file_id=1QpfDSNgSKchIRUqo1WTqa71V0DYc7TCMqaicj1PJAoU#1)**

## ğŸ“ License

Academic project for QuantUniversity â€” Spring 2026
